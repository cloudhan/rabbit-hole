{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import flax.linen as nn\n",
    "\n",
    "from flax.linen.attention import make_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention_weights(q: jnp.ndarray,\n",
    "                                  k: jnp.ndarray,\n",
    "                                  mask: Optional[jnp.ndarray] = None,\n",
    "                                  dtype = jnp.float32):\n",
    "  \"\"\"Computes dot-product attention weights given query and key.\n",
    "\n",
    "  Args:\n",
    "    q: queries for calculating attention with shape of [num_q,  qk_depth].\n",
    "    k: keys for calculating attention with shape of      [num_kv, qk_depth].\n",
    "    mask: mask for the attention weights. This should be of shape [num_q, num_kv].\n",
    "      This can be used for incorporating causal masks.\n",
    "      Attention weights are masked out if their corresponding mask value is `False`.\n",
    "  \"\"\"\n",
    "  assert q.ndim == k.ndim == 2, 'q, k must have rank 2.'\n",
    "  assert q.shape[-1] == k.shape[-1], 'q, k depths must match.'\n",
    "\n",
    "  depth = q.shape[-1]\n",
    "  q = q / jnp.sqrt(depth).astype(dtype)\n",
    "  attn_weights = jnp.einsum('...qd,...kd->...qk', q, k, precision=None)\n",
    "\n",
    "  # apply attention mask\n",
    "  if mask is not None:\n",
    "    big_neg = jnp.finfo(dtype).min  # -3.4028235e+38 for jnp.float32\n",
    "    attn_weights = jnp.where(mask, attn_weights, big_neg)\n",
    "\n",
    "  attn_weights = jax.nn.softmax(attn_weights).astype(dtype)\n",
    "\n",
    "  return attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm(x):\n",
    "    x = x - jnp.mean(x, axis=-1, keepdims=True)\n",
    "    stddev = jnp.sqrt(jnp.mean(x * x, axis=-1, keepdims=True))\n",
    "    return x/stddev\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "  \"\"\"Simple implementation for single head decoder layer, the QKV projections \n",
    "  are omitted for simplicity.\"\"\"\n",
    "  embed_dim: int\n",
    "  ffn_dim: int\n",
    "  \n",
    "  @nn.compact\n",
    "  def __call__(\n",
    "    self,\n",
    "    x: jnp.ndarray,\n",
    "    attention_mask: jnp.ndarray,\n",
    "    encoder_kv: jnp.ndarray,\n",
    "    # encoder_attention_mask: jnp.ndarray,\n",
    "  ):\n",
    "    assert x.ndim == 2 and attention_mask.ndim == 1\n",
    "    assert encoder_kv.ndim == 2 # and encoder_attention_mask.ndim == 1\n",
    "    assert x.shape[0] == attention_mask.shape[0] and x.shape[1] == embed_dim\n",
    "    # assert encoder_kv.shape[0] == encoder_attention_mask.shape[0] and encoder_kv.shape[1] == embed_dim\n",
    "\n",
    "    # 1. self attention part\n",
    "    residual = x\n",
    "    q, k, v = x,x,x\n",
    "    casual_mask = make_causal_mask(attention_mask)[0] == 1.0\n",
    "    attn_weights = dot_product_attention_weights(q, k, casual_mask)\n",
    "    x = attn_weights @ v\n",
    "    # omit a dropout\n",
    "    x = x + residual\n",
    "    x = layernorm(x)\n",
    "\n",
    "    # 2. cross attention part\n",
    "    residual = x\n",
    "    q = x\n",
    "    k, v = encoder_kv, encoder_kv\n",
    "    attn_weights = dot_product_attention_weights(q, k)\n",
    "    x = attn_weights @ v\n",
    "    # omit a dropout\n",
    "    x = x + residual\n",
    "    x = layernorm(x)\n",
    "\n",
    "    # 3. FFN part\n",
    "    residual = x\n",
    "    fc1_weight = self.param(\"fc1\", nn.initializers.normal(), (self.embed_dim, self.ffn_dim))\n",
    "    x = nn.gelu(x @ fc1_weight)\n",
    "    # omit a dropout\n",
    "    fc2_weight = self.param(\"fc2\", nn.initializers.normal(), (self.ffn_dim, self.embed_dim))\n",
    "    x = x @ fc2_weight\n",
    "    x = x + residual\n",
    "    x = layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "embed_dim = 8\n",
    "ffn_dim = 64\n",
    "\n",
    "encoder_kv = random.normal(key=random.PRNGKey(0), shape=(seq_len, embed_dim))\n",
    "# encoder_attention_mask = jnp.ones((seq_len, ), dtype=jnp.int32)\n",
    "\n",
    "# input with seq_len, aka, decoding input at t+1\n",
    "input_feature_2 = random.normal(key=random.PRNGKey(1), shape=(seq_len, embed_dim))\n",
    "attention_mask_2 = jnp.ones((seq_len,), dtype=jnp.int32)\n",
    "\n",
    "# input with seq_len-1, aka, decoding input at t\n",
    "input_feature_1 = input_feature_2[..., :-1, :]\n",
    "attention_mask_1 = jnp.ones((seq_len-1,), dtype=jnp.int32)\n",
    "\n",
    "decoder_layer = DecoderLayer(embed_dim=embed_dim, ffn_dim=ffn_dim)\n",
    "params = decoder_layer.init(\n",
    "    random.PRNGKey(2),\n",
    "    x=jnp.zeros_like(input_feature_2),\n",
    "    attention_mask=jnp.ones_like(attention_mask_2),\n",
    "    encoder_kv=jnp.zeros_like(input_feature_2),\n",
    "    # encoder_attention_mask=jnp.ones_like(attention_mask_2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = decoder_layer.apply(\n",
    "  params,\n",
    "  x=input_feature_1,\n",
    "  attention_mask=attention_mask_1,\n",
    "  encoder_kv=encoder_kv,\n",
    "  # encoder_attention_mask=encoder_attention_mask\n",
    ")\n",
    "output_2 = decoder_layer.apply(\n",
    "  params, \n",
    "  x=input_feature_2, \n",
    "  attention_mask=attention_mask_2, \n",
    "  encoder_kv=encoder_kv, \n",
    "#   encoder_attention_mask=encoder_attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_shape(a, shape):\n",
    "  shape_to_pad = list(a.shape)\n",
    "  shape = list(shape)\n",
    "  pad_width = [(0, s2 - s1) for s1, s2 in zip(shape_to_pad, shape)]\n",
    "  return jnp.pad(a, pad_width=pad_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape at t  : (4, 8)\n",
      "output shape at t+1: (5, 8)\n",
      "diff of output t+1 and t:\n",
      " [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.2758741  -1.9854703  -1.0166407   0.7927936   0.82887286  0.33593562\n",
      "   0.0909228   1.2294604 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"output shape at t  :\", output_1.shape)\n",
    "print(\"output shape at t+1:\", output_2.shape)\n",
    "print(\"diff of output t+1 and t:\\n\", output_2 - pad_to_shape(output_1, output_2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice that for a single decoder layer, the output at decoding step $t$ and $t+1$ is **incremental**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dot_product_attention_weight` implements the most basic $$W_{\\mathrm{attn}} = \\mathrm{softmax}(\\mathrm{mask} \\odot \\frac{QK^\\top}{\\sqrt{d_k}})$$\n",
    "\n",
    "In the case of **lower triangle** causal mask is presented, the final $W_{\\mathrm{attn}}$ will be of the following form\n",
    "$$\n",
    "W_{\\mathrm{attn}} = \n",
    "\\begin{pmatrix}\n",
    "w_{1,1} &         &        &           &   0    \\\\\n",
    "w_{2,1} & w_{2,2} &        &           &        \\\\\n",
    "w_{3,1} & w_{3,2} & \\ddots &           &        \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\ddots    &        \\\\\n",
    "w_{n,1} & w_{n,2} & \\ldots & w_{n,m-1} &w_{n,m}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where $n$ is the number of query entries and $m$ is the number of key and value entries. `num_q` and `num_kv` in code, respectively.\n",
    "\n",
    "In practice, `num_q` and `num_kv` is the number of decoding step $t$.\n",
    "$$\n",
    "W_{\\mathrm{attn}}^t = \n",
    "\\begin{pmatrix}\n",
    "w_{1,1} &        &           &   0    \\\\\n",
    "w_{2,1} & \\ddots &           &        \\\\\n",
    "\\vdots  & \\ddots & \\ddots    &        \\\\\n",
    "w_{t,1} & \\ldots & w_{t,t-1} &w_{t,t}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\newcommand{\\horzbar}{\\rule[.5ex]{2.5ex}{0.5pt}}\n",
    "\\begin{equation}\n",
    "W_{\\mathrm{attn}}^{t+1} = \\begin{pmatrix}\n",
    "W_{\\mathrm{attn}}^t             & 0 \\\\\n",
    "\\horzbar \\, w_{t+1, 1..t} \\, \\horzbar & w_{t+1, t+1}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For $V^t$ and $V^{t+1}$, the value at decoding step $t$ and $t+1$, respectively.\n",
    "$$\n",
    "\\newcommand{\\horzbar}{\\rule[.5ex]{2.5ex}{0.5pt}}\n",
    "V^{t+1} = \n",
    "\\begin{pmatrix}\n",
    "V^t \\\\\n",
    "\\horzbar\\, v^{t+1}\\,\\horzbar\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then the attention output for \n",
    "$$\n",
    "Y^{t+1} = W_{\\mathrm{attn}}^{t+1} V^{t+1} = \n",
    "\\begin{pmatrix}\n",
    "W_{\\mathrm{attn}}^t V^t  \\\\\n",
    "w^{t+1} V^{t+1}\n",
    "\\end{pmatrix} \\overset{?}{=} \\begin{pmatrix}\n",
    "Y^{t}  \\\\\n",
    "w^{t+1} V^{t+1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Notice the recursive structure $W_{\\mathrm{attn}}^{t+1} V^{t+1}$ and $W_{\\mathrm{attn}}^t V^t$, this is where the attention cache comes from. Let's call this form as the **incremental invariant**, the **invariant** for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why and when self attention can be cached?\n",
    "\n",
    "To be able to use cache mechanism, the $\\overset{?}{=}$ must hold for the whole encoder layer because there are computation before and after the attention.\n",
    "\n",
    "Fortunately, it holds because the layer normalization and FFN applies for the embedding/depth dimension, that is, they applies to each position separately and identically. So when a new position is appended from $t \\rightarrow t+1$, the output positions for $1..t$ will not be affected by the new $v^{t+1}$, because of the 0 column in Eq. 1.\n",
    "\n",
    "The proceeding discussion shows that when a lower triangle mask is presented, you can leverage the caching mechanism for the self attention in the first decoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why and when cross attention can be cached?\n",
    "\n",
    "As for the cross attention, the $K$ and $V$ is **static** because they are outputed by the encoder, which will not be updated when deocding step advances from $t \\rightarrow t+1$. Since the self attention preserves the **invariant** \n",
    "$$\n",
    "Y^{t+1} = \\begin{bmatrix} Y^t \\\\ y^{t+1} \\end{bmatrix} = \\mathrm{SelfAttn}(X^{t+1}) = \\mathrm{SelfAttn}(\\begin{bmatrix} X^t \\\\ x^{t+1} \\end{bmatrix})\n",
    "$$\n",
    "which makes the input to the cross attention is the same form as the embedding, a new embedding is appended to the value $V$, a new row is appended to the $Y$ similarly. Then the (NB., $\\frac{\\cdot}{\\sqrt{d_k}}$ is omitted)\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z^{t+1} &= \\mathrm{CrossAttn}(Y^{t+1}) \\\\\n",
    "        &= \\mathrm{softmax}(Y^{t+1} K^\\top) V \\\\\n",
    "        &= \\mathrm{softmax}(\\begin{bmatrix} Y^t \\\\ y^{t+1} \\end{bmatrix} K^\\top) V \\\\\n",
    "        &= \\begin{bmatrix} \\mathrm{softmax}(Y^t K^\\top) \\\\ \\mathrm{softmax}(y^{t+1} K^\\top)  \\end{bmatrix} V \\\\\n",
    "        &= \\begin{bmatrix} \\mathrm{softmax}(Y^t K^\\top) V \\\\ \\mathrm{softmax}(y^{t+1} K^\\top) V \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix} Z^t V \\\\ z^{t+1} V \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "So the **incremental invariant** is also preserved for cross attention if the mask is not presented. It will also hold when the mask is static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can easily show that the FFN part preserve the **invariant**. Finally, the whole (first) decoder layer preserves the **invariant**, this can also be verified by the preceeding program output. Then you can easily see that the **invariant** will also hold for the later decoder layers. That is, all attention calculuation can be cached if Eq. 1 holds. This implies a special form of the mask for the self attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note\n",
    "\n",
    "This shows that the fc1 and fc2 in the `DecoderLayer` implementation can be made of incremental with a proper cache. Then all the GEMM can be reduced to GEMV. That is, we can optimize the decoding process to have a lower order of computational complexity. But it is not seen in the current [transformers decoding](https://github.com/huggingface/transformers) implementation, only attention cache is implemented. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7191e43845b33839613022f0de4c7f68c8bc59d70bdaafc732ea61917499c6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
